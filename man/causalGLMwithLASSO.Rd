% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/causalGLM.R
\name{causalGLMwithLASSO}
\alias{causalGLMwithLASSO}
\title{causalGLMwithLASSO
causalGLM in high dimensions. A wrapper for causalGLM with main-term \code{glmnet}/LASSO as base learner. 
This method is useful for high dimensional settings where other learners are slow or poorly behaved.
It may also be useful for smaller sample sizes where other machine-learning algorithms may overfit.
Otherwise, we do not recommend using this function for lower dimensional settings since glmnet can be mispecified.}
\usage{
causalGLMwithLASSO(
  formula,
  W,
  A,
  Y,
  estimand = c("CATE", "OR", "RR"),
  cross_fit = TRUE,
  weights = NULL,
  data_list = NULL
)
}
\arguments{
\item{formula}{A R formula object specifying the parametric form of CATE, OR, or RR (depending on method).}

\item{W}{A named matrix or data.frame of baseline covariates to condition on.}

\item{A}{A binary treatment assignment vector}

\item{Y}{n outcome variable (continuous, nonnegative or binary depending on method)}

\item{estimand}{Estimand/parameter to estimate. Choices are:
CATE: Estimate conditional average treatment effect with \code{spCATE} assuming it satisfies parametric model \code{formula}.
OR: Estimate conditional odds ratio with \code{spOR} assuming it satisfies parametric model \code{formula}.
OR: Estimate conditional relative risk with \code{spRR} assuming it satisfies parametric model \code{formula}.}

\item{cross_fit}{Whether to cross-fit the initial estimator. By default, TRUE. 
In lower dimensions, we recommend setting this to FALSE.}

\item{weights}{An optional vector of weights to use in procedure.}

\item{data_list}{A named list containing the arguments `W`, `A` and `Y`. For example, data_list = list(W = data[,c("W1", "W2")], A = data[,"A"], Y = data[,"Y"])}
}
\description{
causalGLMwithLASSO
causalGLM in high dimensions. A wrapper for causalGLM with main-term \code{glmnet}/LASSO as base learner. 
This method is useful for high dimensional settings where other learners are slow or poorly behaved.
It may also be useful for smaller sample sizes where other machine-learning algorithms may overfit.
Otherwise, we do not recommend using this function for lower dimensional settings since glmnet can be mispecified.
}
